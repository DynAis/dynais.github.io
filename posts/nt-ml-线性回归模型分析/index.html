<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>线性回归模型分析 | DynAis</title><meta name=keywords content="lr,线性回归"><meta name=description content="线性回归是回归问题中的一种，线性回归假设目标值与特征之间线性相关，即满足一个多元一次方程。通过构建损失函数，来求解损失函数最小时的参数 $w$ 和$b$。通常我们可以表达成如下公式：
$$
\hat{y} = wX + b
$$
$\hat{y}$ 为预测值，自变量 $x$ 和因变量 $y$ 是已知的，而我们想实现的是预测新增一个 $x$ ，其对应的 $y$ 是多少。因此，为了构建这个函数关系，目标是通过已知数据点，求解线性模型中 $w$ 和 $b$ 两个参数。

1.线性回归概述

线性回归是回归问题中的一种，线性回归假设目标值与特征之间线性相关，即满足一个多元一次方程。通过构建损失函数，来求解损失函数最小时的参数 $w$ 和$b$。通常我们可以表达成如下公式：
$$
\begin{equation}
\hat{y} = wX + b \tag{1}
\end{equation}
$$
$\hat{y}$ 为预测值，自变量 $x$ 和因变量 $y$ 是已知的，而我们想实现的是预测新增一个 $x$ ，其对应的 $y$ 是多少。因此，为了构建这个函数关系，目标是通过已知数据点，求解线性模型中 $w$ 和 $b$ 两个参数。
为此, 线性回归分为两个部分, 向前传播和向后传播, 向前传播负责验证当前参数下对数据的拟合程度, 而向后传播通过在向前传播内得到的数据对参数进行优化
"><meta name=author content><link rel=canonical href=https://dynais.xyz/posts/nt-ml-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E5%88%86%E6%9E%90/><link crossorigin=anonymous href=/assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.2840b7fccd34145847db71a290569594bdbdb00047097f75d6495d162f5d7dff.js integrity="sha256-KEC3/M00FFhH23GikFaVlL29sABHCX911kldFi9dff8=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://dynais.xyz/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://dynais.xyz/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://dynais.xyz/favicon-32x32.png><link rel=apple-touch-icon href=https://dynais.xyz/apple-touch-icon.png><link rel=mask-icon href=https://dynais.xyz/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="线性回归模型分析"><meta property="og:description" content="线性回归是回归问题中的一种，线性回归假设目标值与特征之间线性相关，即满足一个多元一次方程。通过构建损失函数，来求解损失函数最小时的参数 $w$ 和$b$。通常我们可以表达成如下公式：
$$
\hat{y} = wX + b
$$
$\hat{y}$ 为预测值，自变量 $x$ 和因变量 $y$ 是已知的，而我们想实现的是预测新增一个 $x$ ，其对应的 $y$ 是多少。因此，为了构建这个函数关系，目标是通过已知数据点，求解线性模型中 $w$ 和 $b$ 两个参数。

1.线性回归概述

线性回归是回归问题中的一种，线性回归假设目标值与特征之间线性相关，即满足一个多元一次方程。通过构建损失函数，来求解损失函数最小时的参数 $w$ 和$b$。通常我们可以表达成如下公式：
$$
\begin{equation}
\hat{y} = wX + b \tag{1}
\end{equation}
$$
$\hat{y}$ 为预测值，自变量 $x$ 和因变量 $y$ 是已知的，而我们想实现的是预测新增一个 $x$ ，其对应的 $y$ 是多少。因此，为了构建这个函数关系，目标是通过已知数据点，求解线性模型中 $w$ 和 $b$ 两个参数。
为此, 线性回归分为两个部分, 向前传播和向后传播, 向前传播负责验证当前参数下对数据的拟合程度, 而向后传播通过在向前传播内得到的数据对参数进行优化
"><meta property="og:type" content="article"><meta property="og:url" content="https://dynais.xyz/posts/nt-ml-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E5%88%86%E6%9E%90/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-02-21T00:00:00+00:00"><meta property="article:modified_time" content="2021-02-21T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="线性回归模型分析"><meta name=twitter:description content="线性回归是回归问题中的一种，线性回归假设目标值与特征之间线性相关，即满足一个多元一次方程。通过构建损失函数，来求解损失函数最小时的参数 $w$ 和$b$。通常我们可以表达成如下公式：
$$
\hat{y} = wX + b
$$
$\hat{y}$ 为预测值，自变量 $x$ 和因变量 $y$ 是已知的，而我们想实现的是预测新增一个 $x$ ，其对应的 $y$ 是多少。因此，为了构建这个函数关系，目标是通过已知数据点，求解线性模型中 $w$ 和 $b$ 两个参数。

1.线性回归概述

线性回归是回归问题中的一种，线性回归假设目标值与特征之间线性相关，即满足一个多元一次方程。通过构建损失函数，来求解损失函数最小时的参数 $w$ 和$b$。通常我们可以表达成如下公式：
$$
\begin{equation}
\hat{y} = wX + b \tag{1}
\end{equation}
$$
$\hat{y}$ 为预测值，自变量 $x$ 和因变量 $y$ 是已知的，而我们想实现的是预测新增一个 $x$ ，其对应的 $y$ 是多少。因此，为了构建这个函数关系，目标是通过已知数据点，求解线性模型中 $w$ 和 $b$ 两个参数。
为此, 线性回归分为两个部分, 向前传播和向后传播, 向前传播负责验证当前参数下对数据的拟合程度, 而向后传播通过在向前传播内得到的数据对参数进行优化
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://dynais.xyz/posts/"},{"@type":"ListItem","position":2,"name":"线性回归模型分析","item":"https://dynais.xyz/posts/nt-ml-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E5%88%86%E6%9E%90/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"线性回归模型分析","name":"线性回归模型分析","description":"线性回归是回归问题中的一种，线性回归假设目标值与特征之间线性相关，即满足一个多元一次方程。通过构建损失函数，来求解损失函数最小时的参数 $w$ 和$b$。通常我们可以表达成如下公式： $$ \\hat{y} = wX + b $$ $\\hat{y}$ 为预测值，自变量 $x$ 和因变量 $y$ 是已知的，而我们想实现的是预测新增一个 $x$ ，其对应的 $y$ 是多少。因此，为了构建这个函数关系，目标是通过已知数据点，求解线性模型中 $w$ 和 $b$ 两个参数。\n1.线性回归概述  线性回归是回归问题中的一种，线性回归假设目标值与特征之间线性相关，即满足一个多元一次方程。通过构建损失函数，来求解损失函数最小时的参数 $w$ 和$b$。通常我们可以表达成如下公式：\n$$ \\begin{equation}\n\\hat{y} = wX + b \\tag{1}\n\\end{equation} $$\n$\\hat{y}$ 为预测值，自变量 $x$ 和因变量 $y$ 是已知的，而我们想实现的是预测新增一个 $x$ ，其对应的 $y$ 是多少。因此，为了构建这个函数关系，目标是通过已知数据点，求解线性模型中 $w$ 和 $b$ 两个参数。\n为此, 线性回归分为两个部分, 向前传播和向后传播, 向前传播负责验证当前参数下对数据的拟合程度, 而向后传播通过在向前传播内得到的数据对参数进行优化\n","keywords":["lr","线性回归"],"articleBody":"线性回归是回归问题中的一种，线性回归假设目标值与特征之间线性相关，即满足一个多元一次方程。通过构建损失函数，来求解损失函数最小时的参数 $w$ 和$b$。通常我们可以表达成如下公式： $$ \\hat{y} = wX + b $$ $\\hat{y}$ 为预测值，自变量 $x$ 和因变量 $y$ 是已知的，而我们想实现的是预测新增一个 $x$ ，其对应的 $y$ 是多少。因此，为了构建这个函数关系，目标是通过已知数据点，求解线性模型中 $w$ 和 $b$ 两个参数。\n1.线性回归概述  线性回归是回归问题中的一种，线性回归假设目标值与特征之间线性相关，即满足一个多元一次方程。通过构建损失函数，来求解损失函数最小时的参数 $w$ 和$b$。通常我们可以表达成如下公式：\n$$ \\begin{equation}\n\\hat{y} = wX + b \\tag{1}\n\\end{equation} $$\n$\\hat{y}$ 为预测值，自变量 $x$ 和因变量 $y$ 是已知的，而我们想实现的是预测新增一个 $x$ ，其对应的 $y$ 是多少。因此，为了构建这个函数关系，目标是通过已知数据点，求解线性模型中 $w$ 和 $b$ 两个参数。\n为此, 线性回归分为两个部分, 向前传播和向后传播, 向前传播负责验证当前参数下对数据的拟合程度, 而向后传播通过在向前传播内得到的数据对参数进行优化\n 2.向前传播  2.1.进行数据预处理 拟合数据之前, 需要对数据先进行预处理\n参数初始化 参数 $w$ 和 $b$ 对初始化没有特别的需求, 置 $0$ 即可\n特征缩放/均一化 对于输入特征值矩阵 $X$ 中数据规模差距大的时候, 应该事先对数据进行特征缩放, 此举的目的是为了在计算中使梯度更快的收敛\n2.2.计算预测结果及损失函数 求解最佳参数，需要一个标准来对结果进行衡量，为此我们需要定量化一个目标函数式，使得计算机可以在求解过程中不断地优化。\n针对任何模型求解问题，都是最终都是可以得到一组预测值 $\\hat{y}$ ，对比已有的真实值 $y$ ，数据行数为 $m$ ，可以将损失函数定义如下： $$ \\begin{equation}\nJ = \\frac{1}{m}\\sum_{i=1}^{m}{(\\hat{y_i} - y_i)^2} \\tag{2}\n\\end{equation} $$ 值 $J$ 就代表着当前模型和数据点的偏移程度\n 3.向后传播  3.1.梯度下降 计算出损失函数之后, 通过使用原有参数减去一个常数与函数求导结果相乘的方式就可以不断地减小损失函数的值, 使参数不断收敛到最佳值, 这种方法在数学上也被称为最小二乘法, 在这里我们称为梯度下降: $$ \\begin{align*}\n\u0026w := w - \\alpha \\cdot dw \\tag{3} \\ \u0026d := d - \\alpha \\cdot dw \\tag{4} \\\n\\end{align*} $$\n3.2.学习速率α 梯度下降算法的每次迭代受到学习率影响, 如果 $\\alpha$ 过小, 则达到收敛所需的迭代次数会非常高; 如果学习率 $\\alpha$ 过大,每次迭代可能不会减小价函数, 可能越局部最大, 导致无法收敛\n使用时可以从小到大成倍增加尝试, 如 $0.01, 0.02, 0.04 …$\n 4.拓展-多项式回归  有时候, 预测结果与输入值之间的关系并非是线性的, 此时就需要使用多项式回归进行拟合\n比如一个二次方模型: $$ \\begin{align*}\n\\hat{y} = w_1 x_1 + w_2 x_2^2 + d \\tag{5}\n\\end{align*} $$ 此时可以使 $$ x_2 = x^2 \\tag{6} $$ 来转化问题成为一个线性回归问题\n 5.拓展-正规方程  通过正规方程可以直接解出向量 $w$, 这种方法适用于样本量不大的情况 $$ w = (X^T X)^{-1} X^T Y \\tag{7} $$\n 6.参考  机器学习 | 算法笔记- 线性回归（Linear Regression）\n","wordCount":"190","inLanguage":"en","datePublished":"2021-02-21T00:00:00Z","dateModified":"2021-02-21T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://dynais.xyz/posts/nt-ml-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E5%88%86%E6%9E%90/"},"publisher":{"@type":"Organization","name":"DynAis","logo":{"@type":"ImageObject","url":"https://dynais.xyz/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://dynais.xyz/ accesskey=h title="DynAis (Alt + H)">DynAis</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>线性回归模型分析</h1><div class=post-meta><span title="2021-02-21 00:00:00 +0000 UTC">February 21, 2021</span></div></header><div class=post-content><p>线性回归是回归问题中的一种，线性回归假设目标值与特征之间线性相关，即满足一个多元一次方程。通过构建损失函数，来求解损失函数最小时的参数 $w$ 和$b$。通常我们可以表达成如下公式：
$$
\hat{y} = wX + b
$$
$\hat{y}$ 为预测值，自变量 $x$ 和因变量 $y$ 是已知的，而我们想实现的是预测新增一个 $x$ ，其对应的 $y$ 是多少。因此，为了构建这个函数关系，目标是通过已知数据点，求解线性模型中 $w$ 和 $b$ 两个参数。</p><h2 id=1线性回归概述>1.线性回归概述<a hidden class=anchor aria-hidden=true href=#1线性回归概述>#</a></h2><hr><p>线性回归是回归问题中的一种，线性回归假设目标值与特征之间线性相关，即满足一个多元一次方程。通过构建损失函数，来求解损失函数最小时的参数 $w$ 和$b$。通常我们可以表达成如下公式：</p><p>$$
\begin{equation}</p><p>\hat{y} = wX + b \tag{1}</p><p>\end{equation}
$$</p><p>$\hat{y}$ 为预测值，自变量 $x$ 和因变量 $y$ 是已知的，而我们想实现的是预测新增一个 $x$ ，其对应的 $y$ 是多少。因此，为了构建这个函数关系，目标是通过已知数据点，求解线性模型中 $w$ 和 $b$ 两个参数。</p><p>为此, 线性回归分为两个部分, 向前传播和向后传播, 向前传播负责验证当前参数下对数据的拟合程度, 而向后传播通过在向前传播内得到的数据对参数进行优化</p><hr><h2 id=2向前传播>2.向前传播<a hidden class=anchor aria-hidden=true href=#2向前传播>#</a></h2><hr><h3 id=21进行数据预处理>2.1.进行数据预处理<a hidden class=anchor aria-hidden=true href=#21进行数据预处理>#</a></h3><p>拟合数据之前, 需要对数据先进行预处理</p><h4 id=参数初始化>参数初始化<a hidden class=anchor aria-hidden=true href=#参数初始化>#</a></h4><p>参数 $w$ 和 $b$ 对初始化没有特别的需求, 置 $0$ 即可</p><h4 id=特征缩放均一化>特征缩放/均一化<a hidden class=anchor aria-hidden=true href=#特征缩放均一化>#</a></h4><p>对于输入特征值矩阵 $X$ 中数据规模差距大的时候, 应该事先对数据进行特征缩放, 此举的目的是为了在计算中使梯度更快的收敛</p><h3 id=22计算预测结果及损失函数>2.2.计算预测结果及损失函数<a hidden class=anchor aria-hidden=true href=#22计算预测结果及损失函数>#</a></h3><p>求解最佳参数，需要一个标准来对结果进行衡量，为此我们需要定量化一个目标函数式，使得计算机可以在求解过程中不断地优化。</p><p>针对任何模型求解问题，都是最终都是可以得到一组预测值 $\hat{y}$ ，对比已有的真实值 $y$ ，数据行数为 $m$ ，可以将损失函数定义如下：
$$
\begin{equation}</p><p>J = \frac{1}{m}\sum_{i=1}^{m}{(\hat{y_i} - y_i)^2} \tag{2}</p><p>\end{equation}
$$
值 $J$ 就代表着当前模型和数据点的偏移程度</p><hr><h2 id=3向后传播>3.向后传播<a hidden class=anchor aria-hidden=true href=#3向后传播>#</a></h2><hr><h3 id=31梯度下降>3.1.梯度下降<a hidden class=anchor aria-hidden=true href=#31梯度下降>#</a></h3><p>计算出损失函数之后, 通过使用原有参数减去一个常数与函数求导结果相乘的方式就可以不断地减小损失函数的值, 使参数不断收敛到最佳值, 这种方法在数学上也被称为<strong>最小二乘法</strong>, 在这里我们称为<strong>梯度下降</strong>:
$$
\begin{align*}</p><p>&w := w - \alpha \cdot dw \tag{3} \
&d := d - \alpha \cdot dw \tag{4} \</p><p>\end{align*}
$$</p><h3 id=32学习速率α>3.2.学习速率α<a hidden class=anchor aria-hidden=true href=#32学习速率α>#</a></h3><p>梯度下降算法的每次迭代受到学习率影响, 如果 <strong>$\alpha$ 过小</strong>, 则<strong>达到收敛所需的迭代次数会非常高</strong>; 如果学习率 <strong>$\alpha$ 过大</strong>,每次迭代可能不会减小价函数, 可能越局部最大, 导致<strong>无法收敛</strong></p><p>使用时可以从小到大成倍增加尝试, 如 $0.01, 0.02, 0.04 &mldr;$</p><hr><h2 id=4拓展-多项式回归>4.拓展-多项式回归<a hidden class=anchor aria-hidden=true href=#4拓展-多项式回归>#</a></h2><hr><p>有时候, 预测结果与输入值之间的关系<strong>并非是线性</strong>的, 此时就需要使用<strong>多项式回归</strong>进行拟合</p><p>比如一个二次方模型:
$$
\begin{align*}</p><p>\hat{y} = w_1 x_1 + w_2 x_2^2 + d \tag{5}</p><p>\end{align*}
$$
此时可以使
$$
x_2 = x^2 \tag{6}
$$
来转化问题成为一个线性回归问题</p><hr><h2 id=5拓展-正规方程>5.拓展-正规方程<a hidden class=anchor aria-hidden=true href=#5拓展-正规方程>#</a></h2><hr><p>通过正规方程可以直接解出向量 $w$, 这种方法适用于样本量不大的情况
$$
w = (X^T X)^{-1} X^T Y \tag{7}
$$</p><hr><h2 id=6参考>6.参考<a hidden class=anchor aria-hidden=true href=#6参考>#</a></h2><hr><p><a href=tps://www.cnblogs.com/geo-will/p/10468253.html>机器学习 | 算法笔记- 线性回归（Linear Regression）</a></p><hr></div><footer class=post-footer><ul class=post-tags><li><a href=https://dynais.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://dynais.xyz/>DynAis</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>